# Enterprise-Data-Pipeline-for-Business-AI

End-to-end, production-style **AI Data Engineering** project focused on **structured enterprise data**:
- **Synthetic business domains**: Customers, Products, Orders, Invoices, Tickets.
- **Data Ops**: Ingestion â†’ Validation â†’ Transformation â†’ Annotation â†’ Feature Store.
- **ML/AI**: Tabular embedding (PCA) for representation learning, **IsolationForest** for anomaly detection, basic churn scoring.
- **Infra**: SQLite demo DB, configurable via YAML, modular code, logging, tests, and an Airflow-like DAG.
- **Deliverables**: Clean Python package, SQL schema, reproducible pipeline, and evaluation reports.

> Designed to mirror tasks in roles like **AI Data Engineer / AI Data Ops / ML Data Engineer** with SAP-style enterprise data.

---

## ðŸ§± Project Layout

```
AI-Data-Engineer-Advanced/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ configs/
â”‚  â””â”€ config.yaml
â”œâ”€ data/
â”‚  â”œâ”€ raw/ (generated)
â”‚  â”œâ”€ interim/ (generated)
â”‚  â”œâ”€ processed/ (generated)
â”‚  â””â”€ models/ (generated)
â”œâ”€ src/
â”‚  â”œâ”€ infra/
â”‚  â”‚  â”œâ”€ db.py
â”‚  â”‚  â””â”€ feature_store.py
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ logging_config.py
â”‚  â”‚  â””â”€ config.py
â”‚  â””â”€ pipelines/
â”‚     â”œâ”€ ingest.py
â”‚     â”œâ”€ validate.py
â”‚     â”œâ”€ transform.py
â”‚     â”œâ”€ annotate.py
â”‚     â”œâ”€ train_embed_and_anomaly.py
â”‚     â””â”€ evaluate.py
â”œâ”€ dags/
â”‚  â””â”€ ai_data_engineer_demo_dag.py
â”œâ”€ tests/
â”‚  â””â”€ test_transform.py
â””â”€ scripts/
   â””â”€ run_all.sh
```

---

## ðŸš€ Quickstart

```bash
# 1) (Optional) Create a venv
python -m venv .venv && source .venv/bin/activate  # on Windows: .venv\Scripts\activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Run end-to-end
bash scripts/run_all.sh
# or step-by-step
python -m src.pipelines.ingest
python -m src.pipelines.validate
python -m src.pipelines.transform
python -m src.pipelines.annotate
python -m src.pipelines.train_embed_and_anomaly
python -m src.pipelines.evaluate
```

Artifacts are written under `data/`:
- `data/raw/*.parquet` â€“ synthetic raw tables
- `data/interim/*.parquet` â€“ validated intermediates
- `data/processed/*.parquet` â€“ features / labels stored here and into SQLite
- `data/models/*` â€“ trained PCA and IsolationForest

---

## ðŸ§ª What this shows (aligned to the JD)

- **Structured business data foundation model (tabular representation):** PCA embeddings capture multi-table signals; easily swappable with TabTransformer or an autoencoder.
- **Data extraction & curation:** Synthetic generation mimics CRM/ERP tables; realistic IDs, datetimes, amounts, currencies, statuses.
- **Pipelines & infra:** Modular ETL with config, logging, and a **feature store** (SQLite) for downstream Data Scientists.
- **Annotation & error analysis:** Rule-based churn proxy, anomaly scores, and explainable KPIs; evaluation report printed to console.
- **Collaboration ready:** Clean interfaces, docstrings, tests, and an **Airflow-style DAG** for orchestration.

---

## ðŸ§° Config

Edit **`configs/config.yaml`** to change sizes, seeds, and thresholds.

---

## ðŸ“ˆ Extending

- Swap PCA with **TabTransformer** or a **PyTorch AutoEncoder** for richer embeddings.
- Replace SQLite with **SAP HANA**, BigQuery, or Snowflake.
- Plug into **Airflow** or **Prefect**; export features to **Feast**.
- Add Great Expectations for data quality rules (row/field-level tests).
- Add CI with `pytest` and pre-commit hooks.

---

## ðŸ“œ License

MIT
